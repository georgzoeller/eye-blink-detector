{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye Blink Detection Framework\n",
    "\n",
    "Implements a blink detection tracking framework based on facial landmarks\n",
    "\n",
    "- Uses dlib (2d detection in Grey Color Space) with the shape_predictor_68_face_landmarks model. [1,4]\n",
    "\n",
    "- Uses mediapipe for debug face mesh output overlay (can be disabled for performance sensitive deployments) [3]\n",
    "\n",
    "\n",
    "## Logic\n",
    "\n",
    "- If eyes closed is detected for NOTIFICATION_SECONDS (e.g. 4), a telegram alert is dispatched\n",
    "- After alert has been dispatched, notifications are blockedun less the user unblocks the system by closing eyes again for NOTIFICATION_SECONDS\n",
    "\n",
    "\n",
    "## Issues\n",
    "\n",
    "- Glasses can interfere with detection\n",
    "- Racial features / skin tones may affect performances.\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "\n",
    "- switch from dlib to mediapipe [2] for detection (works in 3d space, doesn't need beefy model download). \n",
    "  - This should provide superior performance, especially for low contrast situations\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```\n",
    "python 3 + pip\n",
    "python-dotenv\n",
    "opencv-python (cv2)\n",
    "mediapipe\n",
    "dlib\n",
    "```\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Create a .env file with \n",
    "\n",
    "```\n",
    "BOT_TOKEN=Your telegram bot token\n",
    "CHAT_ID=Your telegram chat recipient\n",
    "```\n",
    "2. Run the notebook\n",
    "\n",
    "3. Profit\n",
    "\n",
    "## References\n",
    "\n",
    "[1] https://medium.com/algoasylum/blink-detection-using-python-737a88893825\n",
    "[2] https://towardsdatascience.com/face-landmark-detection-using-python-1964cb620837\n",
    "[3] https://www.analyticsvidhya.com/blog/2021/07/facial-landmark-detection-simplified-with-opencv/\n",
    "[4] https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NOTIFICATION_SECONDS = 4\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path  # Python 3.6+ only\n",
    "env_path = '.env.local'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "bot_token = os.environ['BOT_TOKEN']\n",
    "bot_chatID = os.environ['CHAT_ID']\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "import mediapipe as mp\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def telegram_bot_sendimage(image):    \n",
    "    params = {'chat_id': bot_chatID}\n",
    "    files = {'photo': image}\n",
    "    resp = requests.post('https://api.telegram.org/bot' + bot_token + '/sendPhoto', params, files=files)\n",
    "    return resp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----Step 1: Use VideoCapture in OpenCV-----\n",
    "\n",
    "BLINK_RATIO_THRESHOLD = 5.7\n",
    "\n",
    "#-----Step 5: Getting to know blink ratio\n",
    "\n",
    "def midpoint(point1 ,point2):\n",
    "    return (point1.x + point2.x)/2,(point1.y + point2.y)/2\n",
    "\n",
    "def euclidean_distance(point1 , point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def get_blink_ratio(eye_points, facial_landmarks):\n",
    "    \n",
    "    #loading all the required points\n",
    "    corner_left  = (facial_landmarks.part(eye_points[0]).x, \n",
    "                    facial_landmarks.part(eye_points[0]).y)\n",
    "    corner_right = (facial_landmarks.part(eye_points[3]).x, \n",
    "                    facial_landmarks.part(eye_points[3]).y)\n",
    "    \n",
    "    center_top    = midpoint(facial_landmarks.part(eye_points[1]), \n",
    "                             facial_landmarks.part(eye_points[2]))\n",
    "    center_bottom = midpoint(facial_landmarks.part(eye_points[5]), \n",
    "                             facial_landmarks.part(eye_points[4]))\n",
    "\n",
    "    #calculating distance\n",
    "    horizontal_length = euclidean_distance(corner_left,corner_right)\n",
    "    vertical_length = euclidean_distance(center_top,center_bottom)\n",
    "\n",
    "    ratio = horizontal_length / vertical_length\n",
    "\n",
    "    return ratio\n",
    "\n",
    "#livestream from the webcam \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "'''in case of a video\n",
    "cap = cv2.VideoCapture(\"__path_of_the_video__\")'''\n",
    "\n",
    "#name of the display window in OpenCV\n",
    "cv2.namedWindow('BlinkDetector')\n",
    "\n",
    "#-----Step 3: Face detection with dlib-----\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mpFaceMesh = mp.solutions.face_mesh\n",
    "faceMesh = mpFaceMesh.FaceMesh(max_num_faces=1)\n",
    "drawSpec = mpDraw.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "#-----Step 4: Detecting Eyes using landmarks in dlib-----\n",
    "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
    "#these landmarks are based on the image above \n",
    "left_eye_landmarks  = [36, 37, 38, 39, 40, 41]\n",
    "right_eye_landmarks = [42, 43, 44, 45, 46, 47]\n",
    "blink_count = 0\n",
    "is_blinking = False\n",
    "start_time = None\n",
    "block_notification = False\n",
    "\n",
    "while True:\n",
    "    #capturing frame\n",
    "    retval, frameC = cap.read()\n",
    "\n",
    "    #exit the application if frame not found\n",
    "    if not retval:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break \n",
    "\n",
    "    #-----Step 2: converting image to grayscale-----\n",
    "    \n",
    "    imgRGB = cv2.cvtColor(frameC, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    results = faceMesh.process(imgRGB)    \n",
    "\n",
    "                       \n",
    "\n",
    "\n",
    "    frame = cv2.cvtColor(frameC, cv2.COLOR_BGR2GRAY)\n",
    "    if results.multi_face_landmarks:\n",
    "        for faceLms in results.multi_face_landmarks:\n",
    "            mpDraw.draw_landmarks(frameC, faceLms,mpFaceMesh.FACEMESH_TESSELATION, drawSpec, drawSpec)\n",
    "\n",
    "    #-----Step 3: Face detection with dlib-----\n",
    "    #detecting faces in the frame \n",
    "    faces,_,_ = detector.run(image = frame, upsample_num_times = 0, \n",
    "                       adjust_threshold = 0.0)\n",
    "\n",
    "\n",
    "    #-----Step 4: Detecting Eyes using landmarks in dlib-----\n",
    "    for face in faces:\n",
    "        \n",
    "        landmarks = predictor(frame, face)\n",
    "\n",
    "\n",
    "        #-----Step 5: Calculating blink ratio for one eye-----\n",
    "        left_eye_ratio  = get_blink_ratio(left_eye_landmarks, landmarks)\n",
    "        right_eye_ratio = get_blink_ratio(right_eye_landmarks, landmarks)\n",
    "        blink_ratio     = (left_eye_ratio + right_eye_ratio) / 2\n",
    "\n",
    "        if blink_ratio > BLINK_RATIO_THRESHOLD:\n",
    "            if not is_blinking: \n",
    "                #Blink detected! Do Something!\n",
    "                blink_count+=1                \n",
    "                start_time = time.time()\n",
    "                is_blinking = True\n",
    "            else:\n",
    "                elapsed = time.time() - start_time\n",
    "                if not block_notification:\n",
    "                    cv2.putText(frameC,\"Alerting caregiver in {:.1f}s\".format(NOTIFICATION_SECONDS - elapsed),(10,50), cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,255),1,cv2.LINE_AA)                    \n",
    "                else:\n",
    "                    cv2.putText(frameC,\"Resetting alert in {:.1f}s\".format(NOTIFICATION_SECONDS - elapsed),(10,50), cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,255),1,cv2.LINE_AA)                    \n",
    "                #blink_count+=1\n",
    "                if elapsed > NOTIFICATION_SECONDS:\n",
    "                    if not block_notification:\n",
    "                        telegram_bot_sendtext(\"Caregiver attention requested!\".format(NOTIFICATION_SECONDS))\n",
    "                        block_notification = True                \n",
    "                    else:\n",
    "                        block_notification = False\n",
    "                    \n",
    "\n",
    "\n",
    "                #telegram_bot_sendtext(\"Blinking detected\")\n",
    "        else:\n",
    "            blink_count = 0\n",
    "            is_blinking = False\n",
    "            if block_notification:\n",
    "                  cv2.putText(frameC,\"* Caregiver Alert Dispatched *\",(20,50), cv2.FONT_HERSHEY_SIMPLEX,0.75,(255,255,255),1,cv2.LINE_AA)    \n",
    "\n",
    "      \n",
    "\n",
    "    cv2.imshow('BlinkDetector', frameC)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "#releasing the VideoCapture object\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61cd9621ac0492e2f64914857b9770b284d18f07bae43c1c0fd4d22d5b8cdd24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
